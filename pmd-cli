#!/usr/bin/env python3

## pymediadump - simple tool to download various media files from sites
## Copyright (c) 2021 moonburnt
##
## This program is free software: you can redistribute it and/or modify
## it under the terms of the GNU General Public License as published by
## the Free Software Foundation, either version 3 of the License, or
## (at your option) any later version.
##
## This program is distributed in the hope that it will be useful,
## but WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
## GNU General Public License for more details.
##
## You should have received a copy of the GNU General Public License
## along with this program.  If not, see https://www.gnu.org/licenses/gpl-3.0.txt

import logging
import argparse
from os import makedirs
from sys import exit
from pymediadump import media_downloader, rule_parser, rule_processor
from time import sleep

DEFAULT_DOWNLOAD_DIRECTORY="./Downloads"
DEFAULT_WAIT_TIME=3
RULES_DIRECTORY = "./rules"
PROGRAM_NAME = "pymediadump"
DEFAULT_USER_AGENT = PROGRAM_NAME

# Custom logger
log = logging.getLogger()
log.setLevel(logging.DEBUG)
#log.setLevel(logging.WARNING)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter(fmt='[%(asctime)s][%(name)s][%(levelname)s] %(message)s', datefmt='%H:%M:%S'))
log.addHandler(handler)

def links_to_file(mylist, filename):
    '''Receives list(data to write to text file) and str(path to file). Creates file with provided name and writes content of list to it, line by line'''

    #right now it overwrites file. Maybe will be better to append to it ('a') or do it dynamically, based on something like global "allow overwrite" flag
    with open(filename, 'w') as f:
        log.debug(f"Writing links to file {filename}")
        for item in mylist:
            log.debug(f"Processing {item}")
            f.writelines(f"{item}\n")
    log.debug(f"Successfully finished writing data to {filename}")

def urls_from_file(filename):
    '''Receives str(path to file), returns list(urls from file. One per line)'''
    with open(filename, 'r') as f:
        log.debug(f"Attempting to parse {filename}")
        #data = f.readlines() #this used to add \n at the end of each entry
        data = f.read().splitlines()

    log.debug(f"Got following urls: {data}")
    return data

def data_processor(download_url):
    '''Receives str(webpage to process). Returns dictionary with original url, referer and links to download files'''
    log.debug(f"Attempting to process {download_url}")

    link_data = {}
    link_data['Webpage_URL'] = download_url

    try:
        html_source, link_data['Referer'] = pmd.get_page_source(download_url)
    except Exception as e:
        log.error(f"Some unfortunate error has happend: {e}")
        print(f"Couldnt fetch provided url. Are you sure your link is correct and you have internet connection?")
        return

    download_data = []
    for rule in matching_rules:
        data = rule_processor.rule_processor(html_source, rule)
        if not data:
            data = []
        download_data += data

    link_data['Download_URLs'] = download_data
    log.debug(f"Returning following data collected from {download_url}: {link_data}")

    return link_data

# argparse shenanigans
ap = argparse.ArgumentParser()
ap.add_argument("url", help="URL (or multiple) of webpage, from which you want to download your media", nargs='*', type=str)
ap.add_argument("-d", "--directory", help="Custom path to downloads directory", type=str)
ap.add_argument("--dryrun", help="Dont download anything - just print what will be downloaded", action="store_true")
ap.add_argument("-w", "--wait", help=f"Amount of seconds of pause between downloads (to avoid getting banned for lots of requests). Default/Minimally allowed = {DEFAULT_WAIT_TIME}", type=int)
ap.add_argument("-ua", "--useragent", help=f"Specify custom user agent. If not set - will use {DEFAULT_USER_AGENT}", type=str)
ap.add_argument("-tf", "--to-file", help="Instead of downloading data - writes it to provided file", type=str)
ap.add_argument("-ff", "--from-file", help="Instead of providing urls as launch arguments - parse them from file. One url per line is supported", type=str)
args = ap.parse_args()

if args.from_file:
    try:
        URLS_TO_PROCESS = urls_from_file(args.from_file)
    except Exception as e:
        log.error(f"An error has happend while trying to open file {args.from_file}: {e}")
        print(f"Couldnt read file {args.from_file}. Are you sure the path is correct and file contains data inside?")
        exit(1)
else:
    URLS_TO_PROCESS = args.url

if not URLS_TO_PROCESS:
    print("Got no links to process! Either specify them with positional arguments or call -h for help")
    exit(1)
else:
    log.debug(f"Attempting to process following urls: {URLS_TO_PROCESS}")

if args.wait and (args.wait > DEFAULT_WAIT_TIME):
    WAIT_TIME = args.wait
else:
    WAIT_TIME = DEFAULT_WAIT_TIME
log.info(f"Pause between downloads has been set to {WAIT_TIME} seconds")

DOWNLOAD_DIRECTORY = args.directory or DEFAULT_DOWNLOAD_DIRECTORY
try:
    makedirs(DOWNLOAD_DIRECTORY, exist_ok=True)
except Exception as e:
    log.error(f"An error has happend while trying to create downloads directory: {e}")
    print(f"Couldnt set downloads directory. Either provided path is incorrect or you have no rights to write into {DOWNLOAD_DIRECTORY}")
    exit(1)
log.info(f"Downloads directory has been set to {DOWNLOAD_DIRECTORY}")

USER_AGENT = args.useragent or DEFAULT_USER_AGENT
pmd = media_downloader.Media_Downloader(USER_AGENT)
log.info(f"User agent has been set to {USER_AGENT}")

print(f"Attempting to get list of available download rules")
known_rule_files = rule_parser.get_files(RULES_DIRECTORY)
valid_rules = rule_parser.verify_rules(known_rule_files)

downloads_data = []
for link in URLS_TO_PROCESS:
    print(f"Processing provided url: {link}")
    matching_rules = rule_parser.get_matching_rules(link, valid_rules)
    if len(matching_rules) == 0:
        print(f"No matching rules has been found for url {link}")
        continue
    downloads_data.append(data_processor(link))
    log.debug(f"Waiting {WAIT_TIME} seconds before processing next url to avoid getting banned for spam")
    sleep(WAIT_TIME)
log.debug(f"Got following data regarding downloads: {downloads_data}")

if downloads_data:
    if not args.to_file:
        for entry in downloads_data:
            for link in entry['Download_URLs']:
                try:
                    print(f"Downloading the file from {link} - depending on size, it may require some time")
                    log.debug(f"Waiting {WAIT_TIME} seconds before download to avoid getting banned for spam")
                    sleep(WAIT_TIME)
                    log.debug(f"Attempting to download {link} to {DOWNLOAD_DIRECTORY} with referer {entry['Referer']}")
                    if not args.dryrun:
                        pmd.download_file(link, DOWNLOAD_DIRECTORY, referer=entry['Referer'])
                except Exception as e:
                    log.error(f"Some unfortunate error has happend: {e}")
                    print("Couldnt download the files :( Please double-check your internet connection and try again")
                    continue #to be replaced with some thing that retries up to X times
    else:
        log.debug(f'Got flag --to-file. Adding urls to list')
        urls = []
        for entry in downloads_data:
            for link in entry['Download_URLs']:
                urls.append(link)
        log.debug(f"Received following urls: {urls}")
        log.debug(f"Attempting to write urls on disk")
        try:
            links_to_file(urls, args.to_file)
        except Exception as e:
            log.error(f"Some unfortunate error has happend: {e}")
            print(f"Unable to write links to file {args.to_file}. Abort")
            exit(1)

print("Done")
exit(0)
